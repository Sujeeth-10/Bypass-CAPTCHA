{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2499,
     "status": "ok",
     "timestamp": 1606206445729,
     "user": {
      "displayName": "Tom Huynh",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gio80Z6G2E3hd2cDFHoMoXQp59SPCIxtjC58Pyt=s64",
      "userId": "07622869006304956004"
     },
     "user_tz": -420
    },
    "id": "sC-FEiGM7aFJ",
    "outputId": "c284ac80-1708-4f89-d412-5ad8ec2dcd24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pathlib\n",
    "import os\n",
    "import fnmatch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "import json\n",
    "\n",
    "# make sure we use tensorflow 2.0\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "# import padding library\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import our model, different layers and activation function \n",
    "from tensorflow.keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "DATASET_ZIP_PATH=\"data/catpcha_images.zip\"\n",
    "import zipfile\n",
    "with zipfile.ZipFile(DATASET_ZIP_PATH, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"captcha_project\")\n",
    "!ls\n",
    "# finding where we are so we dont need to type absolute path everytime\n",
    "# current_directory_path = pathlib.Path(\".\").absolute()\n",
    "current_directory_path = pathlib.Path(\"./captcha_project\")\n",
    "current_directory_path\n",
    "# path to our directory images\n",
    "\n",
    "# our raw folders\n",
    "RAW_FOLDER = os.path.join(current_directory_path, \"raw\")\n",
    "print(RAW_FOLDER)\n",
    "# checking the total alphabet characters of the whole dataset\n",
    "raw_data_path = pathlib.Path(RAW_FOLDER)\n",
    "char_list= set()\n",
    "dict_file_label={}\n",
    "for item in raw_data_path.glob('*'):\n",
    "    label = os.path.basename(os.path.splitext(item)[0])\n",
    "    dict_file_label[str(item)]=label\n",
    "    char_list.update(set((label)))\n",
    "char_list=sorted(char_list)\n",
    "\n",
    "print(\"Total number of characters : {}\".format(len(char_list)))\n",
    "# show all possible labels characters\n",
    "\"\".join(char_list)\n",
    "# find the maximum label length\n",
    "label_lens= []\n",
    "for label in dict_file_label.values():\n",
    "    label_lens.append(len(label))\n",
    "max_label_len = max(label_lens)\n",
    "# Only because this dataset is CAPTCHA so the length of all labels are 4.\n",
    "max_label_len\n",
    "# convert the words to array of indexs based on the char_list\n",
    "def encode_to_labels(txt):\n",
    "    # encoding each output word into digits of indexes\n",
    "    dig_lst = []\n",
    "    for index, char in enumerate(txt):\n",
    "        try:\n",
    "            dig_lst.append(char_list.index(char))\n",
    "        except:\n",
    "            print(\"No found in char_list :\", char)\n",
    "        \n",
    "    return dig_lst\n",
    "# testing our encode function (text to number)\n",
    "encode_to_labels(\"2345ABCDE\") \n",
    "# all possible image paths for training\n",
    "all_image_paths = list(dict_file_label.keys())\n",
    "all_image_paths[:5]\n",
    "# find all widths and heights of images (this is useful if our dataset images got different sizes)\n",
    "widths = []\n",
    "heights = []\n",
    "for image_path in all_image_paths:\n",
    "    img = cv2.imread(image_path)\n",
    "    (height, width, _) = img.shape\n",
    "    heights.append(height)\n",
    "    widths.append(width)\n",
    "min_height = min(heights)\n",
    "max_height = max(heights)\n",
    "min_width = min(widths)\n",
    "max_width = max(widths)\n",
    "# this information is useful for making decision for padding and resizing\n",
    "(min_height, max_height, min_width, max_width)\n",
    "# being a good Data Scientist, we need to have train set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.2\n",
    "train_image_paths, test_image_paths = train_test_split(all_image_paths, test_size=test_size, random_state=42)\n",
    "# Constants (we choose this number as we later discover that the output of our CNN is (1,9,512) from out input)\n",
    "TIME_STEPS = 9\n",
    "# TO DO LIST: BUILD THE PIPELINE FOR THE IMAGES, definitely this is super basic pipeline and can be improved\n",
    "\n",
    "# lists for training dataset\n",
    "training_img = []\n",
    "training_txt = []\n",
    "train_input_length = []\n",
    "train_label_length = []\n",
    "orig_txt = []\n",
    "\n",
    "i=0\n",
    "for train_img_path in train_image_paths:\n",
    "    # print(f_name)\n",
    "    # read input image and convert into gray scale image\n",
    "    img = cv2.cvtColor(cv2.imread(train_img_path), cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # in this dataset, we don't need to do any resize at all here.\n",
    "    \n",
    "    # add channel dimension\n",
    "    img = np.expand_dims(img , axis = 2)\n",
    "    \n",
    "    # Normalize each image\n",
    "    img = img/255.\n",
    "    \n",
    "    label = dict_file_label[train_img_path]\n",
    "\n",
    "    # split data into validation and training dataset as 10% and 90% respectively\n",
    "    orig_txt.append(label)   \n",
    "    train_label_length.append(len(label))\n",
    "\n",
    "    # our time steps for valid input\n",
    "    train_input_length.append(TIME_STEPS)\n",
    "    training_img.append(img)\n",
    "\n",
    "    # convert words to digits based on charlist\n",
    "    training_txt.append(encode_to_labels(label)) \n",
    "    i+=1\n",
    "    if (i%500 == 0):\n",
    "        print (\"has processed trained {} files\".format(i))\n",
    "#lists for validation dataset\n",
    "valid_img = []\n",
    "valid_txt = []\n",
    "valid_input_length = []\n",
    "valid_label_length = []\n",
    "valid_orig_txt = []\n",
    "\n",
    "i=0\n",
    "\n",
    "for test_img_path in test_image_paths:\n",
    "    # print(f_name)\n",
    "    # read input image and convert into gray scale image\n",
    "    img = cv2.cvtColor(cv2.imread(test_img_path), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # in this dataset, we don't need to do any resize at all here.\n",
    "    \n",
    "    # add channel dimension\n",
    "    img = np.expand_dims(img , axis = 2)\n",
    "    \n",
    "    # Normalize each image\n",
    "    img = img/255.\n",
    "\n",
    "    label = dict_file_label[test_img_path]\n",
    "\n",
    "    valid_orig_txt.append(label)   \n",
    "    valid_label_length.append(len(label))\n",
    "\n",
    "    # our time steps for valid input\n",
    "    valid_input_length.append(TIME_STEPS)\n",
    "    valid_img.append(img)\n",
    "\n",
    "    # convert words to digits based on charlist\n",
    "    valid_txt.append(encode_to_labels(label))\n",
    "    i+=1\n",
    "    if (i%500 == 0):\n",
    "        print (\"has processed test {} files\".format(i))\n",
    "# this is the most controversial part when our max_label_len should be set different or slightly smaller than TIME_STEPS\n",
    "# but let stick to the conventional/normal way: we should to be the same with our TIME_STEPS\n",
    "max_label_len = TIME_STEPS\n",
    "# pad each output label to maximum text length, remember we did that so that we keep training with rnn consistent?\n",
    "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = 0)\n",
    "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = 0)\n",
    "# OUR FULL MODEL OF CRNN AND LSTM\n",
    "\n",
    "# input with shape of height=32 and width=128 \n",
    "inputs = Input(shape=(24,72,1))\n",
    " \n",
    "# convolution layer with kernel size (3,3)  #(24,72,64)\n",
    "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
    "\n",
    "\n",
    "# poolig layer with kernel size (2,2) to make the height/2 and width/2  #(12,36,64)\n",
    "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
    "\n",
    "#(12,36,128)\n",
    "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
    "\n",
    "# poolig layer with kernel size (2,2) to make the height/2 and width/2  #(6,18,128)\n",
    "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
    "\n",
    "#(6,18,256)\n",
    "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
    "\n",
    "# poolig layer with kernel size (2,2) to make the height/2  #(3,9,256)\n",
    "pool_3 = MaxPool2D(pool_size=(2, 2))(conv_3)\n",
    "\n",
    "batch_norm_3 = BatchNormalization()(pool_3)\n",
    "\n",
    "#(3,9,256)\n",
    "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(batch_norm_3)\n",
    "\n",
    "# Batch normalization layer #(3,9,256)\n",
    "batch_norm_5 = BatchNormalization()(conv_4)\n",
    "\n",
    "#(3,9,512)\n",
    "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
    "batch_norm_6 = BatchNormalization()(conv_6)\n",
    "\n",
    "# poolig layer with kernel size (2,2) to make the height/2 #(1,9,512)\n",
    "pool_6 = MaxPool2D(pool_size=(3, 1))(batch_norm_6)\n",
    " \n",
    "# # to remove the first dimension of one: (1,9,512) -> (9,512)\n",
    "squeezed = Lambda(lambda x: K.squeeze(x, 1))(pool_6)\n",
    " \n",
    "# # # bidirectional LSTM layers with units=128\n",
    "blstm_1 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(squeezed)\n",
    "blstm_2 = Bidirectional(LSTM(256, return_sequences=True, dropout = 0.2))(blstm_1)\n",
    "\n",
    "# # this is our softmax character proprobility with timesteps \n",
    "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
    "\n",
    "# model to be used at test time\n",
    "\n",
    "act_model = Model(inputs, outputs)\n",
    "act_model.summary()\n",
    "\n",
    "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
    "\n",
    "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    " \n",
    "\n",
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "   \n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    " \n",
    "# out loss function (just take the inputs and put it in our ctc_batch_cost)\n",
    "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
    "\n",
    "#model to be used at training time\n",
    "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)\n",
    "# ready ctc loss function and optimizers\n",
    "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
    "\n",
    "# our callbacks hell to optimize our learning\n",
    "callbacks = [\n",
    "    TensorBoard(\n",
    "        log_dir='./logs',\n",
    "        histogram_freq=10,\n",
    "        profile_batch=0,\n",
    "        write_graph=True,\n",
    "        write_images=False,\n",
    "        update_freq=\"epoch\"),\n",
    "    ModelCheckpoint(\n",
    "        filepath='checkpoint_weights.weights.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        verbose=1),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=1e-8,\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        min_delta=1e-8,\n",
    "        factor=0.2,\n",
    "        patience=10,\n",
    "        verbose=1)\n",
    "]\n",
    "callbacks_list = callbacks\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# ready our training data\n",
    "training_img = np.array(training_img)\n",
    "train_input_length = np.array(train_input_length)  # all must be equal length to T timesteps\n",
    "train_label_length = np.array(train_label_length)  # different length (only the same in Captcha dataset)\n",
    "\n",
    "# ready our test data\n",
    "valid_img = np.array(valid_img)\n",
    "valid_input_length = np.array(valid_input_length) # all must be equal length to T timesteps\n",
    "valid_label_length = np.array(valid_label_length) # different length (only the same in Captcha dataset)\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], \n",
    "          y=np.zeros(len(training_img)),\n",
    "          batch_size=batch_size, \n",
    "          epochs = epochs,\n",
    "          validation_data = ([valid_img, valid_padded_txt, valid_input_length, valid_label_length], [np.zeros(len(valid_img))]),\n",
    "          verbose = 1, callbacks = callbacks_list)\n",
    "# save our model\n",
    "model.save('lastest_model.h5')\n",
    "# load the saved best model weights\n",
    "act_model.load_weights('lastest_model.h5')\n",
    "# predict outputs on validation images\n",
    "NO_PREDICTS = 200\n",
    "OFFSET=0\n",
    "prediction = act_model.predict(valid_img[OFFSET:OFFSET+NO_PREDICTS])\n",
    "#prediction = act_model.predict(valid_img)\n",
    "valid_img.shape\n",
    "\n",
    "prediction.shape\n",
    "# use CTC decoder\n",
    "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
    "                         greedy=True)[0][0])\n",
    " \n",
    "# see the results\n",
    "all_predictions =[]\n",
    "i = 0\n",
    "for x in out:\n",
    "    print(\"original_text  = \", valid_orig_txt[i+OFFSET])\n",
    "    print(\"predicted text = \", end = '')\n",
    "    pred = \"\"\n",
    "    for p in x:  \n",
    "        if int(p) != -1:\n",
    "            pred += char_list[int(p)]\n",
    "    print(pred)\n",
    "    all_predictions.append(pred)\n",
    "    i+=1\n",
    "# WRITE TO PRINT OUT THE IMAGES IN NICE 2D ARRAY PLOT WITH ORIGINAL TEXT AND PREDICTED TEXT \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "plt.figure(figsize=(10,100))\n",
    "col=0\n",
    "row=1\n",
    "gs1 = gridspec.GridSpec(NO_PREDICTS//5, 5)\n",
    "gs1.update(wspace=0.025, hspace=0.025)\n",
    "for n in range(NO_PREDICTS):\n",
    "    plt.subplot(gs1[n])\n",
    "    plt.title(\"True:\"+valid_orig_txt[n+OFFSET], fontsize=15, color=\"green\")\n",
    "    plt.imshow(valid_img[n][:,:,0], cmap=\"gray\")\n",
    "    plt.xlabel(\"Pred:\"+all_predictions[n+OFFSET], fontsize=15, color=\"purple\")\n",
    "\n",
    "import string\n",
    "import unicodedata\n",
    "import editdistance\n",
    "\n",
    "def ocr_metrics(predicts, ground_truth, norm_accentuation=False, norm_punctuation=False):\n",
    "    \"\"\"Calculate Character Error Rate (CER), Word Error Rate (WER) and Sequence Error Rate (SER)\"\"\"\n",
    "\n",
    "    if len(predicts) == 0 or len(ground_truth) == 0:\n",
    "        return (1, 1, 1)\n",
    "\n",
    "    cer, wer, ser = [], [], []\n",
    "\n",
    "    for (pd, gt) in zip(predicts, ground_truth):\n",
    "\n",
    "        if norm_accentuation:\n",
    "            pd = unicodedata.normalize(\"NFKD\", pd).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "            gt = unicodedata.normalize(\"NFKD\", gt).encode(\"ASCII\", \"ignore\").decode(\"ASCII\")\n",
    "\n",
    "        if norm_punctuation:\n",
    "            pd = pd.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            gt = gt.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "        pd_cer, gt_cer = list(pd.lower()), list(gt.lower())\n",
    "        dist = editdistance.eval(pd_cer, gt_cer)\n",
    "        cer.append(dist / (max(len(pd_cer), len(gt_cer))))\n",
    "\n",
    "        pd_wer, gt_wer = pd.lower().split(), gt.lower().split()\n",
    "        dist = editdistance.eval(pd_wer, gt_wer)\n",
    "        wer.append(dist / (max(len(pd_wer), len(gt_wer))))\n",
    "\n",
    "        pd_ser, gt_ser = [pd], [gt]\n",
    "        dist = editdistance.eval(pd_ser, gt_ser)\n",
    "        ser.append(dist / (max(len(pd_ser), len(gt_ser))))\n",
    "\n",
    "    cer_f = sum(cer) / len(cer)\n",
    "    wer_f = sum(wer) / len(wer)\n",
    "    ser_f = sum(ser) / len(ser)\n",
    "\n",
    "    return (cer_f, wer_f, ser_f)\n",
    "evaluate = ocr_metrics(predicts=all_predictions,\n",
    "                                  ground_truth=valid_orig_txt,\n",
    "                                  norm_accentuation=False,\n",
    "                                  norm_punctuation=False)\n",
    "\n",
    "e_corpus = \"\\n\".join([\n",
    "    \"Metrics:\",\n",
    "    \"Character Error Rate: {}\".format(evaluate[0]),\n",
    "    \"Word Error Rate:      {}\".format(evaluate[1]),\n",
    "    \"Sequence Error Rate:  {}\".format(evaluate[2]),\n",
    "])\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Choose the index of the image you want to predict\n",
    "i = 20 # Change to any other index if needed\n",
    "\n",
    "# Get the image and add batch dimension\n",
    "img = np.expand_dims(valid_img[i], axis=0)  # shape becomes (1, H, W, 1)\n",
    "\n",
    "# Predict\n",
    "prediction = act_model.predict(img)\n",
    "\n",
    "# Decode using CTC\n",
    "decoded_out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(1)*prediction.shape[1], greedy=True)[0][0])\n",
    "\n",
    "# Convert to text\n",
    "pred_text = ''\n",
    "for p in decoded_out[0]:\n",
    "    if int(p) != -1:\n",
    "        pred_text += char_list[int(p)]\n",
    "\n",
    "# Show results\n",
    "print(\"Original text: \", valid_orig_txt[i])\n",
    "print(\"Predicted text:\", pred_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CRNN_captcha.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
